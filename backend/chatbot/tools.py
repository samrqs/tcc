import asyncio
import logging
from typing import List, Type
from urllib.parse import urljoin, urlparse

import requests
from asgiref.sync import sync_to_async
from bs4 import BeautifulSoup
from django.db import connection
from langchain.tools import BaseTool
from pydantic import BaseModel, Field

from .config import OPENWEATHER_API_KEY
from .metrics import track_error, track_rag_search, track_weather_search
from .vectorstore import get_vectorstore

logger = logging.getLogger(__name__)


class RAGSearchInput(BaseModel):
    """Input para a ferramenta RAG Search."""

    query: str = Field(description="Pergunta ou consulta para buscar nos documentos")
    k: int = Field(
        default=3,
        description="N√∫mero m√°ximo de documentos relevantes a retornar (padr√£o = 3)",
    )


class RAGSearchTool(BaseTool):
    """Ferramenta para buscar informa√ß√µes nos documentos RAG."""

    name: str = "rag_search"
    description: str = """
    Busque informa√ß√µes relevantes nos documentos da base de conhecimento.
    Use esta ferramenta para responder perguntas gerais ou encontrar trechos de refer√™ncia.
    
    Exemplos de uso:
    - "Como plantar milho?"
    - "Quais pr√°ticas de irriga√ß√£o existem?"
    - "√öltimas t√©cnicas para controle natural de pragas"
    """
    args_schema: Type[BaseModel] = RAGSearchInput

    def _run(self, query: str, k: int = 3) -> str:
        """Busca informa√ß√µes nos documentos RAG."""
        logger.info(f"RAG Search iniciado - Query: '{query}', k: {k}")

        try:
            # Track RAG search
            track_rag_search("general")

            logger.debug("Obtendo vectorstore...")
            vectorstore = get_vectorstore()
            retriever = vectorstore.as_retriever(search_kwargs={"k": k})
            logger.debug("Vectorstore obtido com sucesso")

            # Busca documentos relevantes
            logger.debug(f"Executando busca por documentos relevantes...")
            docs = retriever.get_relevant_documents(query)
            logger.info(f"RAG Search - Documentos encontrados: {len(docs)}")

            if not docs:
                logger.warning(
                    f"RAG Search - Nenhum documento encontrado para query: '{query}'"
                )
                return "N√£o foram encontradas informa√ß√µes relevantes nos documentos."

            # Formata as informa√ß√µes encontradas
            results = []
            logger.debug("Formatando resultados encontrados...")
            for i, doc in enumerate(docs, 1):
                content = doc.page_content.strip()
                original_length = len(content)

                logger.debug(
                    f"Resultado {i}: conte√∫do completo com {original_length} caracteres"
                )

                results.append(f"Resultado {i}:\n{content}\n")

            if not results:
                logger.warning("RAG Search - Nenhum resultado formatado dispon√≠vel")
                return "N√£o foram encontradas informa√ß√µes relevantes nos documentos."

            logger.info(
                f"RAG Search conclu√≠do com sucesso - Query: '{query}', Resultados: {len(results)}"
            )

            return "\n\n".join(results)

        except Exception as e:
            # Track error
            track_error("rag_search_error", "tools")
            logger.error(
                f"Erro no RAG Search - Query: '{query}', Erro: {str(e)}", exc_info=True
            )
            return f"Erro ao buscar nos documentos: {str(e)}"

    async def _arun(self, query: str, k: int = 3) -> str:
        """Vers√£o ass√≠ncrona da busca."""
        return self._run(query, k)


class WeatherInput(BaseModel):
    """Input para a ferramenta Weather."""

    location: str = Field(
        default="Parelheiros,SP,BR",
        description="Localiza√ß√£o para consultar o clima (cidade, estado, pa√≠s)",
    )


class WeatherTool(BaseTool):
    """Ferramenta para consultar informa√ß√µes meteorol√≥gicas."""

    name: str = "weather_search"
    description: str = """
    Consulte informa√ß√µes meteorol√≥gicas atuais para uma localiza√ß√£o espec√≠fica.
    Use esta ferramenta para obter dados sobre temperatura, umidade, press√£o atmosf√©rica 
    e condi√ß√µes clim√°ticas que podem afetar a agricultura.
    
    Exemplos de uso:
    - "Qual o clima atual em Parelheiros?"
    - "Est√° chovendo na regi√£o?"
    - "Qual a umidade do ar hoje?"
    """
    args_schema: Type[BaseModel] = WeatherInput

    def _run(self, location: str = "Parelheiros,SP,BR") -> str:
        """Consulta informa√ß√µes meteorol√≥gicas."""
        try:
            # Track weather search
            track_weather_search(location)

            if not OPENWEATHER_API_KEY:
                track_error("missing_api_key", "weather_tool")
                return "API key do OpenWeatherMap n√£o configurada. Entre em contato com o administrador."

            # URL da API OpenWeatherMap
            url = f"http://api.openweathermap.org/data/2.5/weather"
            params = {
                "q": location,
                "appid": OPENWEATHER_API_KEY,
                "units": "metric",  # Celsius
                "lang": "pt_br",  # Portugu√™s brasileiro
            }

            response = requests.get(url, params=params, timeout=10)

            if response.status_code == 401:
                track_error("api_auth_error", "weather_tool")
                return "Erro de autentica√ß√£o na API meteorol√≥gica. Verifique a chave da API."

            if response.status_code == 404:
                track_error("location_not_found", "weather_tool")
                return f"Localiza√ß√£o '{location}' n√£o encontrada. Tente com o nome de uma cidade v√°lida."

            if response.status_code != 200:
                track_error("api_request_error", "weather_tool")
                return f"Erro ao consultar dados meteorol√≥gicos: {response.status_code}"

            data = response.json()

            # Extrai informa√ß√µes relevantes
            main = data.get("main", {})
            weather = data.get("weather", [{}])[0]
            wind = data.get("wind", {})

            city_name = data.get("name", location)
            country = data.get("sys", {}).get("country", "")

            temperature = main.get("temp", 0)
            feels_like = main.get("feels_like", 0)
            humidity = main.get("humidity", 0)
            pressure = main.get("pressure", 0)

            description = weather.get("description", "").title()
            wind_speed = wind.get("speed", 0)

            # Formata a resposta
            weather_info = f"""üå§Ô∏è Clima em {city_name}, {country}

üå°Ô∏è Temperatura: {temperature:.1f}¬∞C (sensa√ß√£o t√©rmica: {feels_like:.1f}¬∞C)
üíß Umidade: {humidity}%
üìä Press√£o atmosf√©rica: {pressure} hPa
üå¨Ô∏è Vento: {wind_speed:.1f} m/s
‚òÅÔ∏è Condi√ß√£o: {description}

üìç Localiza√ß√£o consultada: {location}
"""

            logger.info(f"Weather Search - Location: {location}, Temp: {temperature}¬∞C")

            return weather_info

        except requests.RequestException as e:
            track_error("connection_error", "weather_tool")
            logger.error(f"Erro na requisi√ß√£o meteorol√≥gica: {e}")
            return "Erro de conex√£o ao consultar dados meteorol√≥gicos. Tente novamente em alguns minutos."
        except Exception as e:
            track_error("weather_tool_error", "weather_tool")
            logger.error(f"Erro na WeatherTool: {e}")
            return f"Erro ao consultar informa√ß√µes meteorol√≥gicas: {str(e)}"

    async def _arun(self, location: str = "Parelheiros,SP,BR") -> str:
        """Vers√£o ass√≠ncrona da consulta meteorol√≥gica."""
        return self._run(location)


class WebScrapingInput(BaseModel):
    """Input para a ferramenta de Web Scraping."""

    url: str = Field(description="URL da p√°gina web para extrair informa√ß√µes")
    selector: str = Field(
        default="",
        description="Seletor CSS opcional para extrair elementos espec√≠ficos (ex: 'h1', '.classe', '#id')",
    )
    extract_links: bool = Field(
        default=False,
        description="Se deve extrair links da p√°gina",
    )


class WebScrapingTool(BaseTool):
    """Ferramenta para extrair informa√ß√µes de p√°ginas web usando BeautifulSoup."""

    name: str = "web_scraping"
    description: str = """
    Extrai informa√ß√µes de p√°ginas web usando BeautifulSoup.
    Use esta ferramenta para obter conte√∫do de sites, not√≠cias sobre agricultura,
    pre√ßos de commodities, informa√ß√µes t√©cnicas ou qualquer conte√∫do web relevante.
    
    Exemplos de uso:
    - "Extrair informa√ß√µes sobre pre√ßos do milho de uma p√°gina"
    - "Buscar not√≠cias sobre agricultura sustent√°vel"
    - "Obter dados t√©cnicos de equipamentos agr√≠colas"
    """
    args_schema: Type[BaseModel] = WebScrapingInput

    def _run(self, url: str, selector: str = "", extract_links: bool = False) -> str:
        """Extrai informa√ß√µes de uma p√°gina web."""
        logger.info(f"Web Scraping iniciado - URL: '{url}', Selector: '{selector}'")

        try:
            # Valida√ß√£o b√°sica da URL
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                return "URL inv√°lida. Por favor, forne√ßa uma URL completa (ex: https://exemplo.com)"

            # Headers para simular um navegador real
            headers = {
                "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }

            # Requisi√ß√£o HTTP
            logger.debug(f"Fazendo requisi√ß√£o para: {url}")
            response = requests.get(url, headers=headers, timeout=15)

            if response.status_code != 200:
                track_error("http_error", "web_scraping")
                return f"Erro HTTP {response.status_code} ao acessar a p√°gina: {url}"

            # Parse do HTML com BeautifulSoup
            logger.debug("Fazendo parse do HTML...")
            soup = BeautifulSoup(response.content, "html.parser")

            # Remove scripts e estilos para um conte√∫do mais limpo
            for script in soup(["script", "style", "nav", "footer", "aside"]):
                script.decompose()

            result_parts = []

            # T√≠tulo da p√°gina
            title = soup.find("title")
            if title:
                result_parts.append(f"üìÑ **T√≠tulo:** {title.get_text().strip()}")

            # Se um seletor espec√≠fico foi fornecido
            if selector:
                logger.debug(f"Aplicando seletor: {selector}")
                elements = soup.select(selector)
                if elements:
                    result_parts.append(f"\nüéØ **Conte√∫do do seletor '{selector}':**")
                    for i, element in enumerate(elements[:5], 1):  # M√°ximo 5 elementos
                        text = element.get_text().strip()
                        if text:
                            result_parts.append(
                                f"{i}. {text[:300]}{'...' if len(text) > 300 else ''}"
                            )
                else:
                    result_parts.append(
                        f"\n‚ùå Nenhum elemento encontrado com o seletor '{selector}'"
                    )
            else:
                # Extra√ß√£o de conte√∫do principal
                main_content = []

                # Tenta encontrar o conte√∫do principal
                main_areas = soup.find_all(
                    ["main", "article", "div"],
                    class_=lambda x: x
                    and any(
                        keyword in x.lower()
                        for keyword in ["content", "main", "article", "post", "body"]
                    ),
                )

                if main_areas:
                    for area in main_areas[:2]:  # M√°ximo 2 √°reas principais
                        text = area.get_text().strip()
                        if len(text) > 50:  # Filtra textos muito pequenos
                            main_content.append(text[:800])  # Limita o tamanho
                else:
                    # Fallback: pega todos os par√°grafos
                    paragraphs = soup.find_all("p")
                    for p in paragraphs[:5]:  # M√°ximo 5 par√°grafos
                        text = p.get_text().strip()
                        if len(text) > 30:
                            main_content.append(text[:400])

                if main_content:
                    result_parts.append("\nüìù **Conte√∫do principal:**")
                    for i, content in enumerate(main_content, 1):
                        result_parts.append(
                            f"\n{i}. {content}{'...' if len(content) >= 400 else ''}"
                        )

            # Extra√ß√£o de links se solicitado
            if extract_links:
                logger.debug("Extraindo links...")
                links = soup.find_all("a", href=True)
                unique_links = []
                seen_urls = set()

                for link in links[:10]:  # M√°ximo 10 links
                    href = link.get("href")
                    text = link.get_text().strip()

                    if href and text and len(text) > 3:
                        # Converte links relativos em absolutos
                        full_url = urljoin(url, href)
                        if full_url not in seen_urls and full_url.startswith(
                            ("http://", "https://")
                        ):
                            seen_urls.add(full_url)
                            unique_links.append(f"‚Ä¢ [{text[:50]}]({full_url})")

                if unique_links:
                    result_parts.append("\nüîó **Links encontrados:**")
                    result_parts.extend(unique_links)

            if not result_parts:
                return "N√£o foi poss√≠vel extrair conte√∫do significativo da p√°gina."

            # Adiciona informa√ß√µes da fonte
            result_parts.append(f"\nüåê **Fonte:** {url}")

            final_result = "\n".join(result_parts)

            # Limita o tamanho total da resposta
            if len(final_result) > 2000:
                final_result = final_result[:2000] + "\n\n[Conte√∫do truncado...]"

            logger.info(
                f"Web Scraping conclu√≠do - URL: {url}, Tamanho: {len(final_result)} chars"
            )
            return final_result

        except requests.RequestException as e:
            track_error("connection_error", "web_scraping")
            logger.error(f"Erro de conex√£o no Web Scraping: {e}")
            return f"Erro de conex√£o ao acessar a p√°gina: {str(e)}"
        except Exception as e:
            track_error("web_scraping_error", "web_scraping")
            logger.error(f"Erro no Web Scraping: {e}", exc_info=True)
            return f"Erro ao extrair informa√ß√µes da p√°gina: {str(e)}"

    async def _arun(
        self, url: str, selector: str = "", extract_links: bool = False
    ) -> str:
        """Vers√£o ass√≠ncrona do web scraping."""
        return self._run(url, selector, extract_links)


class SQLSelectInput(BaseModel):
    """Input para a ferramenta SQL SELECT."""

    query: str = Field(
        description="Query SQL SELECT para executar (use %s para par√¢metros)"
    )
    params: List = Field(
        default=[],
        description="Lista de par√¢metros para a query (substitui %s na ordem)",
    )


class SQLSelectTool(BaseTool):
    """Ferramenta para executar queries SELECT no banco de dados PostgreSQL."""

    name: str = "sql_select"
    description: str = """
    Realiza uma query no Postgres para buscar informa√ß√µes sobre as tabelas do banco de dados.
    
    S√≥ pode realizar opera√ß√µes de busca (SELECT), n√£o √© permitido a gera√ß√£o de qualquer opera√ß√£o de escrita.
    
    Tabelas dispon√≠veis:
    
    sensors_sensordata:
    - id (bigint): ID √∫nico do registro
    - timestamp (timestamp with time zone): Data/hora da leitura
    - umidade (double precision): Umidade do solo (%)
    - condutividade (double precision): Condutividade el√©trica (¬µS/cm)
    - temperatura (double precision): Temperatura (¬∞C)
    - ph (double precision): pH do solo
    - nitrogenio (double precision): Nitrog√™nio (ppm)
    - fosforo (double precision): F√≥sforo (ppm)
    - potassio (double precision): Pot√°ssio (ppm)
    - salinidade (double precision): Salinidade (ppm)
    - tds (double precision): Total de s√≥lidos dissolvidos (ppm)
    
    IMPORTANTE: 
    - Apenas queries SELECT s√£o permitidas por motivos de seguran√ßa
    - Todas opera√ß√µes retornam um m√°ximo de 50 itens
    - Use %s para par√¢metros e forne√ßa a lista de valores no campo params
    
    Exemplos de uso:
    - query: "SELECT timestamp, temperatura, umidade FROM sensors_sensordata WHERE timestamp > %s", params: ["2025-01-01"]
    - query: "SELECT COUNT(*) FROM sensors_sensordata WHERE ph BETWEEN %s AND %s", params: [6.0, 8.0]
    """
    args_schema: Type[BaseModel] = SQLSelectInput

    def _validate_query(self, query: str) -> bool:
        """Valida se a query √© segura e √© apenas um SELECT."""
        # Remove espa√ßos e converte para min√∫sculo
        clean_query = query.strip().lower()

        # Verifica se come√ßa com SELECT
        if not clean_query.startswith("select"):
            return False

        # Lista de palavras proibidas que podem ser perigosas
        forbidden_keywords = [
            "insert",
            "update",
            "delete",
            "drop",
            "create",
            "alter",
            "truncate",
            "exec",
            "execute",
            "sp_",
            "xp_",
            "--",
            ";--",
            "into outfile",
            "load_file",
            "information_schema",
            "pg_",
            "current_setting",
            "set ",
            "show ",
            "copy ",
            "\\",
            "pg_read_file",
            "pg_ls_dir",
        ]

        # Verifica se cont√©m palavras proibidas usando any() para melhor performance
        if any(keyword in clean_query for keyword in forbidden_keywords):
            return False

        # Valida√ß√£o adicional: n√£o permitir m√∫ltiplas queries (SQL injection)
        if clean_query.count(";") > 1:
            return False

        # N√£o permitir UNION sem ser em subquery controlada
        if "union" in clean_query and not ("(" in clean_query and ")" in clean_query):
            return False

        return True

    def _format_results(self, results: List[tuple], columns: List[str]) -> str:
        """Formata os resultados da query em uma string leg√≠vel."""
        if not results:
            return "Nenhum resultado encontrado."

        # Para exibi√ß√£o, limita a 20 resultados para n√£o sobrecarregar a resposta
        max_display = 20
        limited_results = results[:max_display]

        # Cabe√ßalho
        header = " | ".join(columns)
        separator = "-" * len(header)
        formatted_lines = [header, separator]

        # Resultados
        for row in limited_results:
            row_str = " | ".join(
                str(value) if value is not None else "NULL" for value in row
            )
            formatted_lines.append(row_str)

        # Informa√ß√µes sobre os resultados
        total_count = len(results)
        if total_count > max_display:
            formatted_lines.append(
                f"\nüìä Mostrando {max_display} de {total_count} resultado(s)"
            )
        else:
            formatted_lines.append(f"\nüìä Total: {total_count} resultado(s)")

        return "\n".join(formatted_lines)

    def _add_limit_to_query(self, query: str) -> str:
        """Adiciona LIMIT 50 √† query se n√£o tiver LIMIT especificado."""
        clean_query = query.strip().lower()

        # Se j√° tem LIMIT, n√£o adiciona outro
        if "limit" in clean_query:
            return query

        # Se √© uma query de COUNT, n√£o adiciona LIMIT
        if clean_query.startswith("select count("):
            return query

        # Adiciona LIMIT 50
        return f"{query.rstrip(';')} LIMIT 50"

    def _execute_query_sync(self, final_query: str, params: List) -> tuple:
        """Executa a query de forma s√≠ncrona e retorna os resultados e colunas."""
        logger.debug("SQL Select Tool - Executando query no banco de dados...")
        with connection.cursor() as cursor:
            if params:
                logger.debug(f"SQL Select Tool - Executando com par√¢metros: {params}")
                cursor.execute(final_query, params)
            else:
                logger.debug("SQL Select Tool - Executando sem par√¢metros")
                cursor.execute(final_query)

            results = cursor.fetchall()
            logger.info(
                f"SQL Select Tool - Query executada com sucesso, {len(results)} registros retornados"
            )

            # Obt√©m os nomes das colunas
            columns = (
                [desc[0] for desc in cursor.description] if cursor.description else []
            )
            logger.debug(f"SQL Select Tool - Colunas retornadas: {columns}")

            return results, columns

    def _run(self, query: str, params: List = None) -> str:
        """Executa a query SQL e retorna os resultados."""
        if params is None:
            params = []

        # Log da query e par√¢metros recebidos
        logger.info(f"SQL Select Tool - Query recebida: {query}")
        logger.info(f"SQL Select Tool - Par√¢metros: {params}")

        try:
            # Valida a query
            if not self._validate_query(query):
                logger.warning(
                    f"SQL Select Tool - Query rejeitada por seguran√ßa: {query}"
                )
                return "Erro: Apenas queries SELECT s√£o permitidas. Query rejeitada por motivos de seguran√ßa."

            # Adiciona LIMIT 50 se necess√°rio
            final_query = self._add_limit_to_query(query)

            # Log da query final se foi modificada
            if final_query != query:
                logger.info(f"SQL Select Tool - Query modificada para: {final_query}")

            # Sempre tenta execu√ß√£o direta primeiro, se falhar usa thread
            try:
                logger.debug("SQL Select Tool - Tentando execu√ß√£o direta")
                results, columns = self._execute_query_sync(final_query, params)
            except Exception as sync_error:
                if "async context" in str(sync_error):
                    logger.debug(
                        "SQL Select Tool - Contexto ass√≠ncrono detectado, executando em thread"
                    )
                    # For√ßa execu√ß√£o em thread separada quando em contexto ass√≠ncrono
                    import concurrent.futures

                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(
                            self._execute_query_sync, final_query, params
                        )
                        results, columns = future.result()
                else:
                    # Re-lan√ßa outros erros
                    raise sync_error

            # Adiciona informa√ß√£o sobre o limite aplicado
            result_text = self._format_results(results, columns)
            if "LIMIT 50" in final_query and final_query != query:
                result_text += "\n\nüìù Nota: LIMIT 50 foi aplicado automaticamente para otimizar a performance."

            logger.info(
                f"SQL Select Tool - Resultado formatado com sucesso ({len(result_text)} caracteres)"
            )
            return result_text

        except Exception as e:
            logger.error(f"SQL Select Tool - Erro ao executar query: {query}")
            logger.error(f"SQL Select Tool - Par√¢metros usados: {params}")
            logger.error(f"SQL Select Tool - Erro detalhado: {str(e)}", exc_info=True)
            track_error("sql_query_error", "sql_select_tool")
            return f"Erro ao executar query: {str(e)}"

    async def _arun(self, query: str, params: List = None) -> str:
        """Vers√£o ass√≠ncrona da execu√ß√£o."""
        if params is None:
            params = []

        # Log da query e par√¢metros recebidos
        logger.info(f"SQL Select Tool - Query recebida: {query}")
        logger.info(f"SQL Select Tool - Par√¢metros: {params}")

        try:
            # Valida a query
            if not self._validate_query(query):
                logger.warning(
                    f"SQL Select Tool - Query rejeitada por seguran√ßa: {query}"
                )
                return "Erro: Apenas queries SELECT s√£o permitidas. Query rejeitada por motivos de seguran√ßa."

            # Adiciona LIMIT 50 se necess√°rio
            final_query = self._add_limit_to_query(query)

            # Log da query final se foi modificada
            if final_query != query:
                logger.info(f"SQL Select Tool - Query modificada para: {final_query}")

            # Executa a query de forma ass√≠ncrona
            results, columns = await sync_to_async(
                self._execute_query_sync, thread_sensitive=True
            )(final_query, params)

            # Adiciona informa√ß√£o sobre o limite aplicado
            result_text = self._format_results(results, columns)
            if "LIMIT 50" in final_query and final_query != query:
                result_text += "\n\nüìù Nota: LIMIT 50 foi aplicado automaticamente para otimizar a performance."

            logger.info(
                f"SQL Select Tool - Resultado formatado com sucesso ({len(result_text)} caracteres)"
            )
            return result_text

        except Exception as e:
            logger.error(f"SQL Select Tool - Erro ao executar query: {query}")
            logger.error(f"SQL Select Tool - Par√¢metros usados: {params}")
            logger.error(f"SQL Select Tool - Erro detalhado: {str(e)}", exc_info=True)
            track_error("sql_query_error", "sql_select_tool")
            return f"Erro ao executar query: {str(e)}"


def get_tools() -> List[BaseTool]:
    """Retorna a lista de ferramentas dispon√≠veis."""
    return [RAGSearchTool(), WeatherTool(), WebScrapingTool(), SQLSelectTool()]
