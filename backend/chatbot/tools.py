import logging
from typing import List, Type
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from langchain.tools import BaseTool
from pydantic import BaseModel, Field

from .config import OPENWEATHER_API_KEY
from .metrics import track_error, track_rag_search, track_weather_search
from .vectorstore import get_vectorstore

logger = logging.getLogger(__name__)


class RAGSearchInput(BaseModel):
    """Input para a ferramenta RAG Search."""

    query: str = Field(description="Pergunta ou consulta para buscar nos documentos")
    k: int = Field(
        default=3,
        description="N√∫mero m√°ximo de documentos relevantes a retornar (padr√£o = 3)",
    )


class RAGSearchTool(BaseTool):
    """Ferramenta para buscar informa√ß√µes nos documentos RAG."""

    name: str = "rag_search"
    description: str = """
    Busque informa√ß√µes relevantes nos documentos da base de conhecimento.
    Use esta ferramenta para responder perguntas gerais ou encontrar trechos de refer√™ncia.
    
    Exemplos de uso:
    - "Como plantar milho?"
    - "Quais pr√°ticas de irriga√ß√£o existem?"
    - "√öltimas t√©cnicas para controle natural de pragas"
    """
    args_schema: Type[BaseModel] = RAGSearchInput

    def _run(self, query: str, k: int = 3) -> str:
        """Busca informa√ß√µes nos documentos RAG."""
        logger.info(f"RAG Search iniciado - Query: '{query}', k: {k}")

        try:
            # Track RAG search
            track_rag_search("general")

            logger.debug("Obtendo vectorstore...")
            vectorstore = get_vectorstore()
            retriever = vectorstore.as_retriever(search_kwargs={"k": k})
            logger.debug("Vectorstore obtido com sucesso")

            # Busca documentos relevantes
            logger.debug(f"Executando busca por documentos relevantes...")
            docs = retriever.get_relevant_documents(query)
            logger.info(f"RAG Search - Documentos encontrados: {len(docs)}")

            if not docs:
                logger.warning(
                    f"RAG Search - Nenhum documento encontrado para query: '{query}'"
                )
                return "N√£o foram encontradas informa√ß√µes relevantes nos documentos."

            # Formata as informa√ß√µes encontradas
            results = []
            logger.debug("Formatando resultados encontrados...")
            for i, doc in enumerate(docs, 1):
                content = doc.page_content.strip()
                original_length = len(content)

                if len(content) > 500:
                    content = content[:500] + "..."
                    logger.debug(
                        f"Resultado {i}: conte√∫do truncado de {original_length} para 500 caracteres"
                    )
                else:
                    logger.debug(
                        f"Resultado {i}: conte√∫do completo com {original_length} caracteres"
                    )

                results.append(f"Resultado {i}:\n{content}\n")

            if not results:
                logger.warning("RAG Search - Nenhum resultado formatado dispon√≠vel")
                return "N√£o foram encontradas informa√ß√µes relevantes nos documentos."

            logger.info(
                f"RAG Search conclu√≠do com sucesso - Query: '{query}', Resultados: {len(results)}"
            )

            return "\n\n".join(results)

        except Exception as e:
            # Track error
            track_error("rag_search_error", "tools")
            logger.error(
                f"Erro no RAG Search - Query: '{query}', Erro: {str(e)}", exc_info=True
            )
            return f"Erro ao buscar nos documentos: {str(e)}"

    async def _arun(self, query: str, k: int = 3) -> str:
        """Vers√£o ass√≠ncrona da busca."""
        return self._run(query, k)


class WeatherInput(BaseModel):
    """Input para a ferramenta Weather."""

    location: str = Field(
        default="Parelheiros,SP,BR",
        description="Localiza√ß√£o para consultar o clima (cidade, estado, pa√≠s)",
    )


class WeatherTool(BaseTool):
    """Ferramenta para consultar informa√ß√µes meteorol√≥gicas."""

    name: str = "weather_search"
    description: str = """
    Consulte informa√ß√µes meteorol√≥gicas atuais para uma localiza√ß√£o espec√≠fica.
    Use esta ferramenta para obter dados sobre temperatura, umidade, press√£o atmosf√©rica 
    e condi√ß√µes clim√°ticas que podem afetar a agricultura.
    
    Exemplos de uso:
    - "Qual o clima atual em Parelheiros?"
    - "Est√° chovendo na regi√£o?"
    - "Qual a umidade do ar hoje?"
    """
    args_schema: Type[BaseModel] = WeatherInput

    def _run(self, location: str = "Parelheiros,SP,BR") -> str:
        """Consulta informa√ß√µes meteorol√≥gicas."""
        try:
            # Track weather search
            track_weather_search(location)

            if not OPENWEATHER_API_KEY:
                track_error("missing_api_key", "weather_tool")
                return "API key do OpenWeatherMap n√£o configurada. Entre em contato com o administrador."

            # URL da API OpenWeatherMap
            url = f"http://api.openweathermap.org/data/2.5/weather"
            params = {
                "q": location,
                "appid": OPENWEATHER_API_KEY,
                "units": "metric",  # Celsius
                "lang": "pt_br",  # Portugu√™s brasileiro
            }

            response = requests.get(url, params=params, timeout=10)

            if response.status_code == 401:
                track_error("api_auth_error", "weather_tool")
                return "Erro de autentica√ß√£o na API meteorol√≥gica. Verifique a chave da API."

            if response.status_code == 404:
                track_error("location_not_found", "weather_tool")
                return f"Localiza√ß√£o '{location}' n√£o encontrada. Tente com o nome de uma cidade v√°lida."

            if response.status_code != 200:
                track_error("api_request_error", "weather_tool")
                return f"Erro ao consultar dados meteorol√≥gicos: {response.status_code}"

            data = response.json()

            # Extrai informa√ß√µes relevantes
            main = data.get("main", {})
            weather = data.get("weather", [{}])[0]
            wind = data.get("wind", {})

            city_name = data.get("name", location)
            country = data.get("sys", {}).get("country", "")

            temperature = main.get("temp", 0)
            feels_like = main.get("feels_like", 0)
            humidity = main.get("humidity", 0)
            pressure = main.get("pressure", 0)

            description = weather.get("description", "").title()
            wind_speed = wind.get("speed", 0)

            # Formata a resposta
            weather_info = f"""üå§Ô∏è Clima em {city_name}, {country}

üå°Ô∏è Temperatura: {temperature:.1f}¬∞C (sensa√ß√£o t√©rmica: {feels_like:.1f}¬∞C)
üíß Umidade: {humidity}%
üìä Press√£o atmosf√©rica: {pressure} hPa
üå¨Ô∏è Vento: {wind_speed:.1f} m/s
‚òÅÔ∏è Condi√ß√£o: {description}

üìç Localiza√ß√£o consultada: {location}
"""

            logger.info(f"Weather Search - Location: {location}, Temp: {temperature}¬∞C")

            return weather_info

        except requests.RequestException as e:
            track_error("connection_error", "weather_tool")
            logger.error(f"Erro na requisi√ß√£o meteorol√≥gica: {e}")
            return "Erro de conex√£o ao consultar dados meteorol√≥gicos. Tente novamente em alguns minutos."
        except Exception as e:
            track_error("weather_tool_error", "weather_tool")
            logger.error(f"Erro na WeatherTool: {e}")
            return f"Erro ao consultar informa√ß√µes meteorol√≥gicas: {str(e)}"

    async def _arun(self, location: str = "Parelheiros,SP,BR") -> str:
        """Vers√£o ass√≠ncrona da consulta meteorol√≥gica."""
        return self._run(location)


class WebScrapingInput(BaseModel):
    """Input para a ferramenta de Web Scraping."""

    url: str = Field(description="URL da p√°gina web para extrair informa√ß√µes")
    selector: str = Field(
        default="",
        description="Seletor CSS opcional para extrair elementos espec√≠ficos (ex: 'h1', '.classe', '#id')",
    )
    extract_links: bool = Field(
        default=False,
        description="Se deve extrair links da p√°gina",
    )


class WebScrapingTool(BaseTool):
    """Ferramenta para extrair informa√ß√µes de p√°ginas web usando BeautifulSoup."""

    name: str = "web_scraping"
    description: str = """
    Extrai informa√ß√µes de p√°ginas web usando BeautifulSoup.
    Use esta ferramenta para obter conte√∫do de sites, not√≠cias sobre agricultura,
    pre√ßos de commodities, informa√ß√µes t√©cnicas ou qualquer conte√∫do web relevante.
    
    Exemplos de uso:
    - "Extrair informa√ß√µes sobre pre√ßos do milho de uma p√°gina"
    - "Buscar not√≠cias sobre agricultura sustent√°vel"
    - "Obter dados t√©cnicos de equipamentos agr√≠colas"
    """
    args_schema: Type[BaseModel] = WebScrapingInput

    def _run(self, url: str, selector: str = "", extract_links: bool = False) -> str:
        """Extrai informa√ß√µes de uma p√°gina web."""
        logger.info(f"Web Scraping iniciado - URL: '{url}', Selector: '{selector}'")

        try:
            # Valida√ß√£o b√°sica da URL
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                return "URL inv√°lida. Por favor, forne√ßa uma URL completa (ex: https://exemplo.com)"

            # Headers para simular um navegador real
            headers = {
                "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }

            # Requisi√ß√£o HTTP
            logger.debug(f"Fazendo requisi√ß√£o para: {url}")
            response = requests.get(url, headers=headers, timeout=15)

            if response.status_code != 200:
                track_error("http_error", "web_scraping")
                return f"Erro HTTP {response.status_code} ao acessar a p√°gina: {url}"

            # Parse do HTML com BeautifulSoup
            logger.debug("Fazendo parse do HTML...")
            soup = BeautifulSoup(response.content, "html.parser")

            # Remove scripts e estilos para um conte√∫do mais limpo
            for script in soup(["script", "style", "nav", "footer", "aside"]):
                script.decompose()

            result_parts = []

            # T√≠tulo da p√°gina
            title = soup.find("title")
            if title:
                result_parts.append(f"üìÑ **T√≠tulo:** {title.get_text().strip()}")

            # Se um seletor espec√≠fico foi fornecido
            if selector:
                logger.debug(f"Aplicando seletor: {selector}")
                elements = soup.select(selector)
                if elements:
                    result_parts.append(f"\nüéØ **Conte√∫do do seletor '{selector}':**")
                    for i, element in enumerate(elements[:5], 1):  # M√°ximo 5 elementos
                        text = element.get_text().strip()
                        if text:
                            result_parts.append(
                                f"{i}. {text[:300]}{'...' if len(text) > 300 else ''}"
                            )
                else:
                    result_parts.append(
                        f"\n‚ùå Nenhum elemento encontrado com o seletor '{selector}'"
                    )
            else:
                # Extra√ß√£o de conte√∫do principal
                main_content = []

                # Tenta encontrar o conte√∫do principal
                main_areas = soup.find_all(
                    ["main", "article", "div"],
                    class_=lambda x: x
                    and any(
                        keyword in x.lower()
                        for keyword in ["content", "main", "article", "post", "body"]
                    ),
                )

                if main_areas:
                    for area in main_areas[:2]:  # M√°ximo 2 √°reas principais
                        text = area.get_text().strip()
                        if len(text) > 50:  # Filtra textos muito pequenos
                            main_content.append(text[:800])  # Limita o tamanho
                else:
                    # Fallback: pega todos os par√°grafos
                    paragraphs = soup.find_all("p")
                    for p in paragraphs[:5]:  # M√°ximo 5 par√°grafos
                        text = p.get_text().strip()
                        if len(text) > 30:
                            main_content.append(text[:400])

                if main_content:
                    result_parts.append("\nüìù **Conte√∫do principal:**")
                    for i, content in enumerate(main_content, 1):
                        result_parts.append(
                            f"\n{i}. {content}{'...' if len(content) >= 400 else ''}"
                        )

            # Extra√ß√£o de links se solicitado
            if extract_links:
                logger.debug("Extraindo links...")
                links = soup.find_all("a", href=True)
                unique_links = []
                seen_urls = set()

                for link in links[:10]:  # M√°ximo 10 links
                    href = link.get("href")
                    text = link.get_text().strip()

                    if href and text and len(text) > 3:
                        # Converte links relativos em absolutos
                        full_url = urljoin(url, href)
                        if full_url not in seen_urls and full_url.startswith(
                            ("http://", "https://")
                        ):
                            seen_urls.add(full_url)
                            unique_links.append(f"‚Ä¢ [{text[:50]}]({full_url})")

                if unique_links:
                    result_parts.append("\nüîó **Links encontrados:**")
                    result_parts.extend(unique_links)

            if not result_parts:
                return "N√£o foi poss√≠vel extrair conte√∫do significativo da p√°gina."

            # Adiciona informa√ß√µes da fonte
            result_parts.append(f"\nüåê **Fonte:** {url}")

            final_result = "\n".join(result_parts)

            # Limita o tamanho total da resposta
            if len(final_result) > 2000:
                final_result = final_result[:2000] + "\n\n[Conte√∫do truncado...]"

            logger.info(
                f"Web Scraping conclu√≠do - URL: {url}, Tamanho: {len(final_result)} chars"
            )
            return final_result

        except requests.RequestException as e:
            track_error("connection_error", "web_scraping")
            logger.error(f"Erro de conex√£o no Web Scraping: {e}")
            return f"Erro de conex√£o ao acessar a p√°gina: {str(e)}"
        except Exception as e:
            track_error("web_scraping_error", "web_scraping")
            logger.error(f"Erro no Web Scraping: {e}", exc_info=True)
            return f"Erro ao extrair informa√ß√µes da p√°gina: {str(e)}"

    async def _arun(
        self, url: str, selector: str = "", extract_links: bool = False
    ) -> str:
        """Vers√£o ass√≠ncrona do web scraping."""
        return self._run(url, selector, extract_links)


def get_tools() -> List[BaseTool]:
    """Retorna a lista de ferramentas dispon√≠veis."""
    return [RAGSearchTool(), WeatherTool(), WebScrapingTool()]
